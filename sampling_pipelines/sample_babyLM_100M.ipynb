{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "SNIPPET_SIZE = 10_000\n",
    "\n",
    "def sample_from_single_file(file_name, target_words, target_folder):\n",
    "    with open(f\"../datasets/BabyLM_dataset/train_100M/{file_name}.train\", \"r\", encoding=\"utf-8\") as f:\n",
    "        words = f.read().split()\n",
    "    total_words = len(words)\n",
    "    num_snippets = int(1.2 * target_words) // SNIPPET_SIZE\n",
    "    if (total_words > target_words + int(0.2 * target_words)):\n",
    "        max_start = total_words - SNIPPET_SIZE\n",
    "        starts = random.sample(range(max_start), num_snippets)\n",
    "\n",
    "        snippets = [words[start:start + SNIPPET_SIZE] for start in starts]\n",
    "\n",
    "        sampled_words = [word for snippet in snippets for word in snippet]\n",
    "\n",
    "        train_words = sampled_words[:target_words]\n",
    "        dev_words = sampled_words[target_words:]\n",
    "\n",
    "        # Fix: Write only train words to main file\n",
    "        with open(f\"../datasets/BabyLM_dataset/{target_folder}/{file_name}.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(\" \".join(train_words))  # Changed from sampled_words to train_words\n",
    "        with open(f\"../datasets/BabyLM_dataset/{target_folder}/{file_name}_dev.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(\" \".join(dev_words))\n",
    "    else: \n",
    "        print(f\"File {file_name} has only {total_words} words, not enough to sample {target_words} words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_proportions (output_name, no_words, bnc_spoken, childes, gutenberg, open_subtitles, simple_wiki, switchboard):\n",
    "    if bnc_spoken + childes + gutenberg + open_subtitles + simple_wiki + switchboard != 1:\n",
    "        raise ValueError(\"Proportions must sum to 1.\")\n",
    "\n",
    "    files = [\"bnc_spoken.train\", \"childes.train\", \"gutenberg.train\", \"open_subtitles.train\", \"simple_wiki.train\", \"switchboard.train\"]\n",
    "    proportions = [bnc_spoken, childes, gutenberg, open_subtitles, simple_wiki, switchboard]\n",
    "\n",
    "    train_words = []\n",
    "    dev_words = []\n",
    "    \n",
    "    for i in range(len(files)):\n",
    "        file = files[i]\n",
    "        total_words_needed = int((no_words + DEV_WORDS) * proportions[i])\n",
    "        train_words_needed = int(no_words * proportions[i])\n",
    "        dev_words_needed = total_words_needed - train_words_needed\n",
    "        \n",
    "        if total_words_needed == 0:\n",
    "            continue\n",
    "            \n",
    "        with open(f\"../datasets/BabyLM_dataset/train_100M/{file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            words = f.read().split()\n",
    "        total_words = len(words)\n",
    "\n",
    "        if total_words < total_words_needed:\n",
    "            print(f\"File {file} has only {total_words} words, not enough to sample {total_words_needed} words.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate snippets needed for this file specificallyNUM_SNIPnum_snippetsPETS\n",
    "        snippets_needed = (total_words_needed + SNIPPET_SIZE - 1) // SNIPPET_SIZE  # Ceiling division\n",
    "        max_start = total_words - SNIPPET_SIZE\n",
    "        starts = random.sample(range(max_start), min(snippets_needed, max_start))\n",
    "        snippets = [words[start:start + SNIPPET_SIZE] for start in starts]\n",
    "        file_words = [word for snippet in snippets for word in snippet][:total_words_needed]\n",
    "        \n",
    "        # Split this file's words into train and dev proportionally\n",
    "        file_train_words = file_words[:train_words_needed]\n",
    "        file_dev_words = file_words[train_words_needed:train_words_needed + dev_words_needed]\n",
    "        \n",
    "        train_words.extend(file_train_words)\n",
    "        dev_words.extend(file_dev_words)\n",
    "    \n",
    "    print(f\"Train words: {len(train_words)}\")\n",
    "    print(f\"Dev words: {len(dev_words)}\")\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{output_name}.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(train_words))\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{output_name}_dev.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(dev_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_percentage_of_books (percentage, target_words, filename):\n",
    "    with open(f\"../datasets/BabyLM_dataset/train_100M/gutenberg.train\", \"r\", encoding=\"utf-8\") as f:\n",
    "        all_books = f.read()\n",
    "    lines = all_books.split(\"\\n\")\n",
    "\n",
    "    # split up the books\n",
    "    texts = {}\n",
    "    beginning_indices = []\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(\"= = = \"):\n",
    "            beginning_indices.append(i)\n",
    "    target_with_dev = int(1.2 * target_words)\n",
    "    # sample the first percentage of books\n",
    "    text = \"\"\n",
    "    total_words = 0\n",
    "    for i in range(int(len(beginning_indices) * percentage)):\n",
    "        number_of_lines = beginning_indices[i + 1] - beginning_indices[i] - 1\n",
    "        last_index = beginning_indices[i] + int(percentage * number_of_lines)\n",
    "        book_text = \" \".join(lines[(beginning_indices[i] + 1):(last_index)])\n",
    "        total_words += len(book_text.split())\n",
    "        text += book_text\n",
    "        if total_words >= target_with_dev:\n",
    "            print(f\"Sampled {total_words} words from {i + 1} books.\")\n",
    "            text_words = text.split()\n",
    "            train_text = \" \".join(text_words[:target_words])\n",
    "            print(f\"Train text length: {len(train_text.split())} words.\")\n",
    "            dev_text = \" \".join(text_words[target_words:target_with_dev])\n",
    "            print(f\"Dev text length: {len(dev_text.split())} words.\")\n",
    "            break\n",
    "\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{filename}.train\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(train_text)\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{filename}_dev.train\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(dev_text)\n",
    "\n",
    "sample_percentage_of_books(0.25, 1_000_000, \"gutenberg_1M_25pct_books\")\n",
    "sample_percentage_of_books(0.5, 1_000_000, \"gutenberg_1M_50pct_books\")\n",
    "sample_percentage_of_books(0.75, 1_000_000, \"gutenberg_1M_75pct_books\")\n",
    "sample_percentage_of_books(1, 1_000_000, \"gutenberg_1M_100pct_books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_single_file(\"gutenberg\", 100_000, \"train_100k\")\n",
    "sample_from_single_file(\"simple_wiki\", 100_000, \"train_100k\")\n",
    "sample_from_single_file(\"open_subtitles\", 100_000, \"train_100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msample_from_single_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgutenberg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m300_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_300k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m sample_from_single_file(\u001b[33m\"\u001b[39m\u001b[33msimple_wiki\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m300_000\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain_300k\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m sample_from_single_file(\u001b[33m\"\u001b[39m\u001b[33mopen_subtitles\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m300_000\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain_300k\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36msample_from_single_file\u001b[39m\u001b[34m(file_name, target_words, target_folder)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (total_words > target_words + \u001b[38;5;28mint\u001b[39m(\u001b[32m0.2\u001b[39m * target_words)):\n\u001b[32m     12\u001b[39m     max_start = total_words - SNIPPET_SIZE\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     starts = \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_start\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_snippets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     snippets = [words[start:start + SNIPPET_SIZE] \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m starts]\n\u001b[32m     17\u001b[39m     sampled_words = [word \u001b[38;5;28;01mfor\u001b[39;00m snippet \u001b[38;5;129;01min\u001b[39;00m snippets \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m snippet]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/random.py:431\u001b[39m, in \u001b[36mRandom.sample\u001b[39m\u001b[34m(self, population, k, counts)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0\u001b[39m <= k <= n:\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSample larger than population or is negative\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m result = \u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\n\u001b[32m    432\u001b[39m setsize = \u001b[32m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m k > \u001b[32m5\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "sample_from_single_file(\"gutenberg\", 300_000, \"train_300k\")\n",
    "sample_from_single_file(\"simple_wiki\", 300_000, \"train_300k\")\n",
    "sample_from_single_file(\"open_subtitles\", 300_000, \"train_300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
