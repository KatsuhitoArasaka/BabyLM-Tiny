{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "SNIPPET_SIZE = 10_000\n",
    "\n",
    "def sample_from_single_file(file_name, target_words, target_folder):\n",
    "    with open(f\"../datasets/BabyLM_dataset/train_100M/{file_name}.train\", \"r\", encoding=\"utf-8\") as f:\n",
    "        words = f.read().split()\n",
    "    total_words = len(words)\n",
    "    num_snippets = int(1.2 * target_words) // SNIPPET_SIZE\n",
    "    if (total_words > target_words + int(0.2 * target_words)):\n",
    "        max_start = total_words - SNIPPET_SIZE\n",
    "        starts = random.sample(range(max_start), num_snippets)\n",
    "\n",
    "        snippets = [words[start:start + SNIPPET_SIZE] for start in starts]\n",
    "\n",
    "        sampled_words = [word for snippet in snippets for word in snippet]\n",
    "\n",
    "        train_words = sampled_words[:target_words]\n",
    "        dev_words = sampled_words[target_words:]\n",
    "\n",
    "        # Fix: Write only train words to main file\n",
    "        with open(f\"../datasets/BabyLM_dataset/{target_folder}/{file_name}.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(\" \".join(train_words))  # Changed from sampled_words to train_words\n",
    "        with open(f\"../datasets/BabyLM_dataset/{target_folder}/{file_name}_dev.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(\" \".join(dev_words))\n",
    "    else: \n",
    "        print(f\"File {file_name} has only {total_words} words, not enough to sample {target_words} words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_proportions (output_name, no_words, bnc_spoken, childes, gutenberg, open_subtitles, simple_wiki, switchboard):\n",
    "    # if bnc_spoken + childes + gutenberg + open_subtitles + simple_wiki + switchboard != 1:\n",
    "    #     raise ValueError(\"Proportions must sum to 1.\")\n",
    "\n",
    "    files = [\"bnc_spoken.train\", \"childes.train\", \"gutenberg.train\", \"open_subtitles.train\", \"simple_wiki.train\", \"switchboard.train\"]\n",
    "    proportions = [bnc_spoken, childes, gutenberg, open_subtitles, simple_wiki, switchboard]\n",
    "\n",
    "    train_words = []\n",
    "    dev_words = []\n",
    "    \n",
    "    for i in range(len(files)):\n",
    "        file = files[i]\n",
    "        total_words_needed = int((1.2 * no_words) * proportions[i])\n",
    "        train_words_needed = int(no_words * proportions[i])\n",
    "        dev_words_needed = total_words_needed - train_words_needed\n",
    "        \n",
    "        if total_words_needed == 0:\n",
    "            continue\n",
    "            \n",
    "        with open(f\"../datasets/BabyLM_dataset/train_100M/{file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            words = f.read().split()\n",
    "        total_words = len(words)\n",
    "\n",
    "        if total_words < total_words_needed:\n",
    "            print(f\"File {file} has only {total_words} words, not enough to sample {total_words_needed} words.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate snippets needed for this file specificallyNUM_SNIPnum_snippetsPETS\n",
    "        snippets_needed = (total_words_needed + SNIPPET_SIZE - 1) // SNIPPET_SIZE  # Ceiling division\n",
    "        max_start = total_words - SNIPPET_SIZE\n",
    "        starts = random.sample(range(max_start), min(snippets_needed, max_start))\n",
    "        snippets = [words[start:start + SNIPPET_SIZE] for start in starts]\n",
    "        file_words = [word for snippet in snippets for word in snippet][:total_words_needed]\n",
    "        \n",
    "        # Split this file's words into train and dev proportionally\n",
    "        file_train_words = file_words[:train_words_needed]\n",
    "        file_dev_words = file_words[train_words_needed:train_words_needed + dev_words_needed]\n",
    "        \n",
    "        train_words.extend(file_train_words)\n",
    "        dev_words.extend(file_dev_words)\n",
    "    \n",
    "    print(f\"Train words: {len(train_words)}\")\n",
    "    print(f\"Dev words: {len(dev_words)}\")\n",
    "    with open(f\"../datasets/BabyLM_dataset/{output_name}.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(train_words))\n",
    "    with open(f\"../datasets/BabyLM_dataset/{output_name}_dev.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(dev_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.12250712250712248,\n",
    "    0.06267806267806267,\n",
    "    0.14102564102564102,\n",
    "    0.4401709401709401,\n",
    "    0.2165242165242165,\n",
    "    0.017094017094017091])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train words: 1199997\n",
      "Dev words: 240000\n"
     ]
    }
   ],
   "source": [
    "sample_proportions(\"/train_1.2M/og_proportions\", 1_200_000, \n",
    "                   0.12250712250712248,\n",
    "    0.06267806267806267,\n",
    "    0.14102564102564102,\n",
    "    0.4401709401709401,\n",
    "    0.2165242165242165,\n",
    "    0.017094017094017092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_percentage_of_books (percentage, target_words, filename):\n",
    "    with open(f\"../datasets/BabyLM_dataset/train_100M/gutenberg.train\", \"r\", encoding=\"utf-8\") as f:\n",
    "        all_books = f.read()\n",
    "    lines = all_books.split(\"\\n\")\n",
    "\n",
    "    # split up the books\n",
    "    texts = {}\n",
    "    beginning_indices = []\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(\"= = = \"):\n",
    "            beginning_indices.append(i)\n",
    "    target_with_dev = int(1.2 * target_words)\n",
    "    # sample the first percentage of books\n",
    "    text = \"\"\n",
    "    total_words = 0\n",
    "    for i in range(int(len(beginning_indices) * percentage)):\n",
    "        number_of_lines = beginning_indices[i + 1] - beginning_indices[i] - 1\n",
    "        last_index = beginning_indices[i] + int(percentage * number_of_lines)\n",
    "        book_text = \" \".join(lines[(beginning_indices[i] + 1):(last_index)])\n",
    "        total_words += len(book_text.split())\n",
    "        text += book_text\n",
    "        if total_words >= target_with_dev:\n",
    "            print(f\"Sampled {total_words} words from {i + 1} books.\")\n",
    "            text_words = text.split()\n",
    "            train_text = \" \".join(text_words[:target_words])\n",
    "            print(f\"Train text length: {len(train_text.split())} words.\")\n",
    "            dev_text = \" \".join(text_words[target_words:target_with_dev])\n",
    "            print(f\"Dev text length: {len(dev_text.split())} words.\")\n",
    "            break\n",
    "\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{filename}.train\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(train_text)\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{filename}_dev.train\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(dev_text)\n",
    "\n",
    "sample_percentage_of_books(0.25, 1_000_000, \"gutenberg_1M_25pct_books\")\n",
    "sample_percentage_of_books(0.5, 1_000_000, \"gutenberg_1M_50pct_books\")\n",
    "sample_percentage_of_books(0.75, 1_000_000, \"gutenberg_1M_75pct_books\")\n",
    "sample_percentage_of_books(1, 1_000_000, \"gutenberg_1M_100pct_books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_single_file(\"gutenberg\", 100_000, \"train_100k\")\n",
    "sample_from_single_file(\"simple_wiki\", 100_000, \"train_100k\")\n",
    "sample_from_single_file(\"open_subtitles\", 100_000, \"train_100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_single_file(\"gutenberg\", 300_000, \"train_300k\")\n",
    "sample_from_single_file(\"simple_wiki\", 300_000, \"train_300k\")\n",
    "sample_from_single_file(\"open_subtitles\", 300_000, \"train_300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_single_file(\"bnc_spoken\", 100_000, \"train_100k\")\n",
    "sample_from_single_file(\"bnc_spoken\", 300_000, \"train_300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
