{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "TARGET_WORDS = 100_000\n",
    "DEV_WORDS = int(0.2 * TARGET_WORDS)\n",
    "SNIPPET_SIZE = 10_000\n",
    "NUM_SNIPPETS = (TARGET_WORDS + DEV_WORDS) // SNIPPET_SIZE\n",
    "\n",
    "def sample_from_single_file(file_name, target_words):\n",
    "    with open(f\"../datasets/BabyLM_dataset/train_100M/{file_name}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        words = f.read().split()\n",
    "    total_words = len(words)\n",
    "\n",
    "    if (total_words > target_words + DEV_WORDS):\n",
    "        max_start = total_words - SNIPPET_SIZE\n",
    "        starts = random.sample(range(max_start), NUM_SNIPPETS)\n",
    "\n",
    "        snippets = [words[start:start + SNIPPET_SIZE] for start in starts]\n",
    "\n",
    "        sampled_words = [word for snippet in snippets for word in snippet]\n",
    "\n",
    "        train_words = sampled_words[:target_words]\n",
    "        dev_words = sampled_words[target_words:]\n",
    "\n",
    "        # Fix: Write only train words to main file\n",
    "        with open(f\"../datasets/BabyLM_dataset/train_100k/{file_name}\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(\" \".join(train_words))  # Changed from sampled_words to train_words\n",
    "        with open(f\"../datasets/BabyLM_dataset/train_100k/{file_name}_dev.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(\" \".join(dev_words))\n",
    "    else: \n",
    "        print(f\"File {file_name} has only {total_words} words, not enough to sample {target_words} words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_proportions (output_name, no_words, bnc_spoken, childes, gutenberg, open_subtitles, simple_wiki, switchboard):\n",
    "    if bnc_spoken + childes + gutenberg + open_subtitles + simple_wiki + switchboard != 1:\n",
    "        raise ValueError(\"Proportions must sum to 1.\")\n",
    "\n",
    "    files = [\"bnc_spoken.train\", \"childes.train\", \"gutenberg.train\", \"open_subtitles.train\", \"simple_wiki.train\", \"switchboard.train\"]\n",
    "    proportions = [bnc_spoken, childes, gutenberg, open_subtitles, simple_wiki, switchboard]\n",
    "\n",
    "    train_words = []\n",
    "    dev_words = []\n",
    "    \n",
    "    for i in range(len(files)):\n",
    "        file = files[i]\n",
    "        total_words_needed = int((no_words + DEV_WORDS) * proportions[i])\n",
    "        train_words_needed = int(no_words * proportions[i])\n",
    "        dev_words_needed = total_words_needed - train_words_needed\n",
    "        \n",
    "        if total_words_needed == 0:\n",
    "            continue\n",
    "            \n",
    "        with open(f\"../datasets/BabyLM_dataset/train_100M/{file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            words = f.read().split()\n",
    "        total_words = len(words)\n",
    "\n",
    "        if total_words < total_words_needed:\n",
    "            print(f\"File {file} has only {total_words} words, not enough to sample {total_words_needed} words.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate snippets needed for this file specifically\n",
    "        snippets_needed = (total_words_needed + SNIPPET_SIZE - 1) // SNIPPET_SIZE  # Ceiling division\n",
    "        max_start = total_words - SNIPPET_SIZE\n",
    "        starts = random.sample(range(max_start), min(snippets_needed, max_start))\n",
    "        snippets = [words[start:start + SNIPPET_SIZE] for start in starts]\n",
    "        file_words = [word for snippet in snippets for word in snippet][:total_words_needed]\n",
    "        \n",
    "        # Split this file's words into train and dev proportionally\n",
    "        file_train_words = file_words[:train_words_needed]\n",
    "        file_dev_words = file_words[train_words_needed:train_words_needed + dev_words_needed]\n",
    "        \n",
    "        train_words.extend(file_train_words)\n",
    "        dev_words.extend(file_dev_words)\n",
    "    \n",
    "    print(f\"Train words: {len(train_words)}\")\n",
    "    print(f\"Dev words: {len(dev_words)}\")\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{output_name}.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(train_words))\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{output_name}_dev.train\", \"w+\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(dev_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_percentage_of_books (percentage, target_words, filename):\n",
    "    with open(f\"../datasets/BabyLM_dataset/train_100M/gutenberg.train\", \"r\", encoding=\"utf-8\") as f:\n",
    "        all_books = f.read()\n",
    "    lines = all_books.split(\"\\n\")\n",
    "\n",
    "    # split up the books\n",
    "    texts = {}\n",
    "    beginning_indices = []\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(\"= = = \"):\n",
    "            beginning_indices.append(i)\n",
    "    target_with_dev = int(1.2 * target_words)\n",
    "    # sample the first percentage of books\n",
    "    text = \"\"\n",
    "    total_words = 0\n",
    "    for i in range(int(len(beginning_indices) * percentage)):\n",
    "        number_of_lines = beginning_indices[i + 1] - beginning_indices[i] - 1\n",
    "        last_index = beginning_indices[i] + int(percentage * number_of_lines)\n",
    "        book_text = \" \".join(lines[(beginning_indices[i] + 1):(last_index)])\n",
    "        total_words += len(book_text.split())\n",
    "        text += book_text\n",
    "        if total_words >= target_with_dev:\n",
    "            print(f\"Sampled {total_words} words from {i + 1} books.\")\n",
    "            text_words = text.split()\n",
    "            train_text = \" \".join(text_words[:target_words])\n",
    "            print(f\"Train text length: {len(train_text.split())} words.\")\n",
    "            dev_text = \" \".join(text_words[target_words:target_with_dev])\n",
    "            print(f\"Dev text length: {len(dev_text.split())} words.\")\n",
    "            break\n",
    "\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{filename}.train\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(train_text)\n",
    "    with open(f\"../datasets/BabyLM_dataset/books_context/{filename}_dev.train\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(dev_text)\n",
    "\n",
    "sample_percentage_of_books(0.25, 1_000_000, \"gutenberg_1M_25pct_books\")\n",
    "sample_percentage_of_books(0.5, 1_000_000, \"gutenberg_1M_50pct_books\")\n",
    "sample_percentage_of_books(0.75, 1_000_000, \"gutenberg_1M_75pct_books\")\n",
    "sample_percentage_of_books(1, 1_000_000, \"gutenberg_1M_100pct_books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gutenberg.train'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msample_from_single_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgutenberg.train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m sample_from_single_file(\u001b[33m\"\u001b[39m\u001b[33msimple_wiki.train\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m100_000\u001b[39m)\n\u001b[32m      3\u001b[39m sample_from_single_file(\u001b[33m\"\u001b[39m\u001b[33mopen_subtitles.train\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m100_000\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36msample_from_single_file\u001b[39m\u001b[34m(file_path, target_words)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_from_single_file\u001b[39m(file_path, target_words):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     files = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.../datasets/BabyLM_dataset/train_100M/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'gutenberg.train'"
     ]
    }
   ],
   "source": [
    "sample_from_single_file(\"gutenberg.train\", 100_000)\n",
    "sample_from_single_file(\"simple_wiki.train\", 100_000)\n",
    "sample_from_single_file(\"open_subtitles.train\", 100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
