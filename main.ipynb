{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0f_3zEEMr4"
      },
      "source": [
        "# main.ipynb\n",
        "This notebook contains some template code to help you with loading/preprocessing the data.\n",
        "\n",
        "We start with some imports and constants.\n",
        "The training data is found in the `data` subfolder.\n",
        "There is also a tokenizer I've trained for you which you can use for the project."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can be executed once — does not depend on the execution environment."
      ],
      "metadata": {
        "id": "WCX9Tayrtu2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 1. Clone the Repository (if required)\n",
        "# ========================================\n",
        "!git clone https://github.com/KatsuhitoArasaka/BabyLM-Tiny.git\n",
        "%cd BabyLM-Tiny\n",
        "\n",
        "# ============================\n",
        "# 2. Set Paths\n",
        "# ============================\n",
        "TRAIN_PATH = './data/train.txt'       # Path to your training data\n",
        "DEV_PATH = './data/dev.txt'           # Path to your validation data\n",
        "SPM_PATH = './data/tokenizer.model'   # Path to your tokenizer model"
      ],
      "metadata": {
        "id": "P7QGcWQwsygr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute after restarting the environment -- when changing the device (CPU ↔ GPU) you need to restart the kernel."
      ],
      "metadata": {
        "id": "4n2o7lgau14Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 3. Install Dependencies\n",
        "# ==========================\n",
        "!pip install transformers datasets wandb --quiet\n",
        "\n",
        "# ===============================\n",
        "# 4. Authentication with wandb\n",
        "# ===============================\n",
        "import wandb\n",
        "wandb.login()  # Enter your API key when prompted\n",
        "\n",
        "# ==========================\n",
        "# 5. Import Libraries and Set Device\n",
        "# ==========================\n",
        "import torch\n",
        "import datasets\n",
        "from functools import partial\n",
        "from datasets import load_dataset\n",
        "from transformers import DebertaV2Tokenizer as Tokenizer\n",
        "# from transformers.models.deberta_v2.tokenization_deberta_v2 import DebertaV2Tokenizer as Tokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Check for GPU availability\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ],
      "metadata": {
        "id": "23L24AQHId8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glUE2dtEEMr8"
      },
      "source": [
        "Here are we load the dataset and tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkhiZZGfEMr9"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 6. Load Dataset and Tokenizer\n",
        "# ============================\n",
        "dataset = datasets.load_dataset('text', data_files={'train': TRAIN_PATH, 'validation': DEV_PATH}) # loads the dataset\n",
        "tokenizer = Tokenizer.from_pretrained(SPM_PATH) # loads the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H36tyN0PEMr9"
      },
      "source": [
        "Next we need to tokenize the data. I've split this into 2 functions, 1 for an encoder model, 1 for a decoder model.\n",
        "These functions are written to take a dictionary of lists, and return the same. This is specifically to allow us to use the `map()` function in the `datasets` library, which we show below later.\n",
        "\n",
        "### Tokenization (Encoder)\n",
        "For encoder tokenization, we will be doing MLM, and masking is applied randomly and dynamically during training. Right now, the function below is a preprocessing step, and thus only applied once. So we skip masking for now, and just say the input and labels are the same.\n",
        "\n",
        "The argument `add_special_tokens` is set to `True` (it is also true by default, but I put it here for clarity), which means that special tokens that go at the start and end of the text will be added. [CLS] and [SEP] are special tokens marking the beginning and end of the text. So, e.g. \"The sky is blue.\" will become \"[CLS] The sky is blue. [SEP]\"\n",
        "\n",
        "### Tokenization (Decoder)\n",
        "For decoder tokenization, we will be doing CLM, so predicting the next token. Therefore we need to offset the labels from the inputs so that the 1st token is used to predict the 2nd, etc. See the paragraph above for an explanation of [CLS] and [SEP]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw-0QO72EMr-"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 7. Tokenization Function (Encoder/Decoder)\n",
        "# ============================\n",
        "def tokenize_encoder(examples, tokenizer):\n",
        "    batch = {\n",
        "        \"input_ids\": [],\n",
        "        \"labels\": [],\n",
        "    }\n",
        "\n",
        "    for example in examples[\"text\"]:\n",
        "        tokens = tokenizer.encode(example, add_special_tokens=True) # will add [CLS] and [SEP]\n",
        "        batch[\"input_ids\"].append(tokens)\n",
        "        batch[\"labels\"].append(tokens)\n",
        "\n",
        "    return batch\n",
        "\n",
        "def tokenize_decoder(examples, tokenizer):\n",
        "    batch = {\n",
        "        \"input_ids\": [],\n",
        "        \"labels\": [],\n",
        "    }\n",
        "\n",
        "    for example in examples[\"text\"]:\n",
        "        tokens = tokenizer.encode(example, add_special_tokens=True) # will add [CLS] and [SEP]\n",
        "        batch[\"input_ids\"].append(tokens[:-1])\n",
        "        batch[\"labels\"].append(tokens[1:])\n",
        "\n",
        "    return batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F94j3RR6EMr-"
      },
      "source": [
        "Now we can apply the tokenization. I show it for `tokenize_encoder` but it is the same for `tokenize_decoder`. First the function needs to only take 1 parameter, which is fine because our tokenizer is constant, so we can just apply `functools.partial`. Next, we apply `map()`, which allows fast parallel preprocessing of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2mZPNAjEMr_"
      },
      "outputs": [],
      "source": [
        "# Apply the tokenization to the dataset\n",
        "tokenize_fn = partial(tokenize_encoder, tokenizer = tokenizer) # need to make the function unary for map()\n",
        "dataset = dataset.map(tokenize_fn, batched = True, num_proc = 4, remove_columns = ['text']) # map works with functions that return a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 8. Setup Model, Training, and Logging (This is the part that may change per model)\n",
        "# ============================\n",
        "\n",
        "# Specific Settings for the Model you choose\n",
        "# Replace this block with model-specific settings\n",
        "model_name = \"microsoft/deberta-v3-small\"  # Example: DeBERTa model for MLM\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)  # Load a Masked Language Model (MLM)\n",
        "\n",
        "# Data collator to support masked language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "# ============================\n",
        "# 9. Setup Training Arguments\n",
        "# ============================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # Directory to save the model\n",
        "    evaluation_strategy=\"steps\",         # Evaluation strategy (can be \"steps\" or \"epoch\")\n",
        "    eval_steps=500,                      # Steps between evaluations\n",
        "    logging_dir=\"./logs\",                # Directory to save logs\n",
        "    logging_steps=100,                   # Steps between logging\n",
        "    report_to=\"wandb\",                   # Log to wandb\n",
        "    run_name=\"deberta-mlm-run\",          # Run name in wandb\n",
        "    per_device_train_batch_size=8,       # Batch size for training\n",
        "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
        "    num_train_epochs=3,                  # Number of training epochs\n",
        "    save_steps=1000,                     # Save model after every 1000 steps\n",
        "    save_total_limit=2,                  # Limit the number of saved models\n",
        "    load_best_model_at_end=True,         # Load the best model at the end of training\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 10. Train the Model\n",
        "# ============================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sU2WHPCixD5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 11. Save the Model\n",
        "# ============================\n",
        "trainer.save_model(\"trained-deberta\")\n",
        "\n",
        "# ============================\n",
        "# 12. Log Progress in wandb\n",
        "# ============================\n",
        "wandb.finish()  # log the final metrics and mark the run as complete"
      ],
      "metadata": {
        "id": "rVSCNJXvSV3i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}