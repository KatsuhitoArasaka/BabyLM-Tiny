{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.ipynb\n",
    "This notebook contains some template code to help you with loading/preprocessing the data.\n",
    "\n",
    "We start with some imports and constants.\n",
    "The training data is found in the `data` subfolder.\n",
    "There is also a tokenizer I've trained for you which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from functools import partial\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2 import DebertaV2Tokenizer as Tokenizer\n",
    "\n",
    "TRAIN_PATH = './data/train.txt'\n",
    "DEV_PATH = './data/dev.txt'\n",
    "SPM_PATH = './data/tokenizer.model'\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are we load the dataset and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('text', data_files={'train': TRAIN_PATH, 'validation': DEV_PATH}) # loads the dataset\n",
    "tokenizer = Tokenizer.from_pretrained(SPM_PATH) # loads the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tokenize the data. I've split this into 2 functions, 1 for an encoder model, 1 for a decoder model. \n",
    "These functions are written to take a dictionary of lists, and return the same. This is specifically to allow us to use the `map()` function in the `datasets` library, which we show below later. \n",
    "\n",
    "### Tokenization (Encoder)\n",
    "For encoder tokenization, we will be doing MLM, and masking is applied randomly and dynamically during training. Right now, the function below is a preprocessing step, and thus only applied once. So we skip masking for now, and just say the input and labels are the same. \n",
    "\n",
    "The argument `add_special_tokens` is set to `True` (it is also true by default, but I put it here for clarity), which means that special tokens that go at the start and end of the text will be added. [CLS] and [SEP] are special tokens marking the beginning and end of the text. So, e.g. \"The sky is blue.\" will become \"[CLS] The sky is blue. [SEP]\"\n",
    "\n",
    "### Tokenization (Decoder)\n",
    "For decoder tokenization, we will be doing CLM, so predicting the next token. Therefore we need to offset the labels from the inputs so that the 1st token is used to predict the 2nd, etc. See the paragraph above for an explanation of [CLS] and [SEP]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_encoder(examples, tokenizer):\n",
    "    batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "\n",
    "    for example in examples[\"text\"]:\n",
    "        tokens = tokenizer.encode(example, add_special_tokens=True) # will add [CLS] and [SEP]\n",
    "        batch[\"input_ids\"].append(tokens)\n",
    "        batch[\"labels\"].append(tokens)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def tokenize_decoder(examples, tokenizer):\n",
    "    batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "\n",
    "    for example in examples[\"text\"]:\n",
    "        tokens = tokenizer.encode(example, add_special_tokens=True) # will add [CLS] and [SEP]\n",
    "        batch[\"input_ids\"].append(tokens[:-1])\n",
    "        batch[\"labels\"].append(tokens[1:])\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the tokenization. I show it for `tokenize_encoder` but it is the same for `tokenize_decoder`. First the function needs to only take 1 parameter, which is fine because our tokenizer is constant, so we can just apply `functools.partial`. Next, we apply `map()`, which allows fast parallel preprocessing of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_fn = partial(tokenize_encoder, tokenizer=tokenizer) # need to make the function unary for map()\n",
    "dataset = dataset.map(tokenize_fn, batched=True, num_proc=4, remove_columns=['text']) # map works with functions that return a dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
