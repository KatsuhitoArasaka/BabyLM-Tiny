{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# check if /datasets/gutenberg/dataset_books_with_metadata.json exists, if so, load it\n",
    "\n",
    "# fetch the text of the books from the Gutenberg dataset (too large to upload to GitHub)\n",
    "with open(f\"./datasets/train_100M/gutenberg.train\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_books = f.read()\n",
    "lines = all_books.split(\"\\n\")\n",
    "\n",
    "# split up the books\n",
    "texts = {}\n",
    "beginning_indices = []\n",
    "for i in range(len(lines)):\n",
    "    if lines[i].startswith(\"= = = \"):\n",
    "        beginning_indices.append(i)\n",
    "\n",
    "for i in range(len(beginning_indices) - 1):\n",
    "    gutenberg_id = lines[beginning_indices[i]][8:].split()[0]\n",
    "    if not gutenberg_id.isdigit():\n",
    "        print(\"Id-length: \", len(gutenberg_id), \"for id:\", gutenberg_id)\n",
    "        continue\n",
    "    text = \" \".join(lines[(beginning_indices[i] + 1):(beginning_indices[i + 1] - 1)])\n",
    "    texts[gutenberg_id] = text\n",
    "print(f\"Found {len(texts)} books in the Gutenberg dataset.\")\n",
    "print(\"Example book text:\", texts.get(\"52018\", \"\")[:500])  # Print first 500 characters of a sample book\n",
    "\n",
    "books_with_metadata = {} # id -> { 'text': str, 'author': str, 'title': str, 'subjects': list, 'bookshelves': list }\n",
    "\n",
    "# if metadata is locally available, load it\n",
    "if os.path.exists(\"./datasets/gutenberg/dataset_books_with_metadata.json\"):\n",
    "    with open(\"./datasets/gutenberg/dataset_books_with_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        books_with_metadata = json.load(f)\n",
    "    print(f\"Loaded {len(books_with_metadata)} books with metadata from local file.\")\n",
    "    print(\"Example book metadata:\", books_with_metadata.get(\"52018\", {}))\n",
    "\n",
    "# otherwise, fetch metadata from Gutendex\n",
    "else: \n",
    "    # Fetch metadata for each book from Gutendex\n",
    "    count = 0\n",
    "    for gutenberg_id, text in texts.items():\n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            print(f\"Processing book ID {gutenberg_id}... ({count}/{len(texts)})\")\n",
    "        try:\n",
    "            response = requests.get(f\"https://gutendex.com/books/{gutenberg_id}\", timeout=30)\n",
    "            response.raise_for_status()\n",
    "            metadata = response.json()\n",
    "\n",
    "            authors = metadata.get(\"authors\", [])\n",
    "            if not authors:\n",
    "                author = \"Unknown\"\n",
    "            else:\n",
    "                author = authors[0].get(\"name\", \"Unknown\")\n",
    "            \n",
    "            books_with_metadata[gutenberg_id] = {\n",
    "                \n",
    "                'author': author,\n",
    "                'title': metadata[\"title\"],\n",
    "                'subjects': metadata[\"subjects\"],\n",
    "                'bookshelves': metadata[\"bookshelves\"]\n",
    "            }\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to get metadata for book {gutenberg_id}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing book {gutenberg_id}: {e}\")\n",
    "    with open(\"./datasets/gutenberg/dataset_books_with_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(books_with_metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(len(books_with_metadata), \"books with metadata found.\")\n",
    "    print(\"Example book metadata:\", books_with_metadata.get(52018, {}))\n",
    "\n",
    "for gutenberg_id, book in books_with_metadata.items():\n",
    "    if gutenberg_id in texts:\n",
    "        book['text'] = texts[gutenberg_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bookshelves = set()\n",
    "all_subjects = set()\n",
    "bookshelve_counts = {}\n",
    "bookshelve_word_counts = {}\n",
    "subject_counts = {}\n",
    "subject_word_counts = {}\n",
    "for book in books_with_metadata.values():\n",
    "    word_count = len(book[\"text\"].split())\n",
    "    for bookshelve in book[\"bookshelves\"]:\n",
    "        all_bookshelves.add(bookshelve)\n",
    "        if bookshelve in bookshelve_counts:\n",
    "            bookshelve_counts[bookshelve] += 1\n",
    "            bookshelve_word_counts[bookshelve] += word_count\n",
    "        else:\n",
    "            bookshelve_counts[bookshelve] = 1\n",
    "            bookshelve_word_counts[bookshelve] = word_count\n",
    "    for subject in book[\"subjects\"]:\n",
    "        all_subjects.add(subject)\n",
    "        if subject in subject_counts:\n",
    "            subject_counts[subject] += 1\n",
    "            subject_word_counts[subject] += word_count\n",
    "        else:\n",
    "            subject_counts[subject] = 1\n",
    "            subject_word_counts[subject] = word_count\n",
    "\n",
    "print(f\"Total unique bookshelves: {len(all_bookshelves)}\")\n",
    "print(f\"Total unique subjects: {len(all_subjects)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bookshelf_list = sorted(list(all_bookshelves))\n",
    "subject_list = sorted(list(all_subjects))\n",
    "# print 10 most common bookshelves and subjects with their counts\n",
    "print(\"\\nMost common bookshelves:\")\n",
    "sorted_bookshelves = sorted(bookshelve_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for bookshelf, count in sorted_bookshelves[:10]:\n",
    "    print(f\"{bookshelf}: {count} books\")\n",
    "print(\"\\nMost common subjects:\")\n",
    "sorted_subjects = sorted(subject_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for subject, count in sorted_subjects[:10]:\n",
    "    print(f\"{subject}: {count} books\")\n",
    "\n",
    "# print 10 longest bookshelves and subjects with their total word counts\n",
    "print(\"\\nLongest bookshelves:\")\n",
    "sorted_bookshelves_by_length = sorted(bookshelve_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for bookshelf, word_count in sorted_bookshelves_by_length[:10]:\n",
    "    print(f\"{bookshelf}: {word_count} words\")\n",
    "print(\"\\nLongest subjects:\")\n",
    "sorted_subjects_by_length = sorted(subject_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "count = 1\n",
    "for subject, word_count in sorted_subjects_by_length:\n",
    "    if word_count < 500_000:\n",
    "        break\n",
    "    print(f\"{count}: {subject}: {word_count} words\")\n",
    "    count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from subjects to books\n",
    "subject_to_book_map = {}\n",
    "for subject in all_subjects:\n",
    "    subject_to_book_map[subject] = []\n",
    "for book_id, book in books_with_metadata.items():\n",
    "    for subject in book[\"subjects\"]:\n",
    "        subject_to_book_map[subject].append(book_id)\n",
    "\n",
    "# look at all pairs of subject, and check how many books they have in common\n",
    "subject_pairs = {}\n",
    "for i in range(len(subject_list)):\n",
    "    for j in range(i + 1, len(subject_list)):\n",
    "        subject1 = subject_list[i]\n",
    "        subject2 = subject_list[j]\n",
    "        common_books = set(subject_to_book_map[subject1]) & set(subject_to_book_map[subject2])\n",
    "        if common_books:\n",
    "            subject_pairs[(subject1, subject2)] = len(common_books)\n",
    "print(\"\\nMost common subject pairs:\")\n",
    "sorted_subject_pairs = sorted(subject_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "for (subject1, subject2), count in sorted_subject_pairs[:10]:\n",
    "    print(f\"{subject1} & {subject2}: {count} common books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a list of tuples (gutenberg_id, name) a filename and a word limit\n",
    "# writes file with the books that have a word count less than the limit to \n",
    "# ./datasets/gutenberg/genres/filename.train\n",
    "def create_dataset_from_list(books, filename, word_limit):\n",
    "    if os.path.exists(f\"./datasets/gutenberg/genres/{filename}.train\"):\n",
    "        os.remove(f\"./datasets/gutenberg/genres/{filename}.train\")\n",
    "    word_count = 0\n",
    "    for gutenberg_id, name in books:\n",
    "        try:\n",
    "            response = requests.get(f\"https://www.gutenberg.org/cache/epub/{gutenberg_id}/pg{gutenberg_id}.txt\", timeout=30)\n",
    "            response.raise_for_status()\n",
    "            text = response.text\n",
    "            if word_count + len(text.split()) > word_limit:\n",
    "                print(f\"Word limit reached. Stopping.\")\n",
    "                # write the remaining number of words to hit the word limit exactly\n",
    "                text = \" \".join(text.split()[:word_limit - word_count])\n",
    "                with open(f\"./datasets/gutenberg/genres/{filename}.train\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"= = = {gutenberg_id} {name}\\n\")\n",
    "                    f.write(text + \"\\n\")\n",
    "                print(f\"Wrote {word_count + len(text.split())} words to {filename}.train with word limit {word_limit}.\")\n",
    "                break\n",
    "            word_count += len(text.split())\n",
    "            with open(f\"./datasets/gutenberg/genres/{filename}.train\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"= = = {gutenberg_id} {name}\\n\")\n",
    "                f.write(text + \"\\n\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to get metadata for book {gutenberg_id}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing book {gutenberg_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list = [\n",
    "    (84, \"Frankenstein; Or, The Modern Prometheus\"),                      # Mary Shelley :contentReference[oaicite:1]{index=1}\n",
    "    (35, \"The Time Machine\"),                                            # H. G. Wells :contentReference[oaicite:2]{index=2}\n",
    "    (36, \"The Island of Doctor Moreau\"),                                 # H. G. Wells :contentReference[oaicite:3]{index=3}\n",
    "    (36, \"The War of the Worlds\"),                                       # H. G. Wells :contentReference[oaicite:4]{index=4}\n",
    "    (5230, \"The Invisible Man\"),                                         # H. G. Wells :contentReference[oaicite:5]{index=5}\n",
    "    (12163, \"The Sleeper Awakes\"),                                       # H. G. Wells (aka When the Sleeper Wakes) :contentReference[oaicite:6]{index=6}\n",
    "    (36, \"When the Sleeper Wakes\"),                                      # duplicate ID but alt title :contentReference[oaicite:7]{index=7}\n",
    "    (1887, \"A Princess of Mars\"),                                        # Edgar Rice Burroughs :contentReference[oaicite:8]{index=8}\n",
    "    (164, \"Twenty Thousand Leagues Under the Sea\"),                      # Jules Verne :contentReference[oaicite:9]{index=9}\n",
    "    (147, \"A Journey to the Centre of the Earth\"),                       # Jules Verne :contentReference[oaicite:10]{index=10}\n",
    "    (120, \"The Mysterious Island\"),                                      # Jules Verne :contentReference[oaicite:11]{index=11}\n",
    "    (16, \"From the Earth to the Moon\"),                                  # Jules Verne :contentReference[oaicite:12]{index=12}\n",
    "    (523, \"Flatland: A Romance of Many Dimensions\"),                     # Edwin A. Abbott :contentReference[oaicite:13]{index=13}\n",
    "    (766, \"Triplanetary\"),                                               # E. E. “Doc” Smith :contentReference[oaicite:14]{index=14}\n",
    "    (526, \"First Lensman\"),                                              # E. E. Smith :contentReference[oaicite:15]{index=15}\n",
    "    (34217, \"The Lost World\"),                                           # Sir Arthur Conan Doyle :contentReference[oaicite:16]{index=16}\n",
    "    (121, \"The Strange Case of Dr. Jekyll and Mr. Hyde\"),                # Stevenson (proto-SF/horror) :contentReference[oaicite:17]{index=17}\n",
    "    (4080, \"2 B R 0 2 B\"),                                               # Kurt Vonnegut :contentReference[oaicite:18]{index=18}\n",
    "    (73886, \"Scanners Live in Vain\"),                                    # Cordwainer Smith :contentReference[oaicite:19]{index=19}\n",
    "    (111, \"Caesar’s Column\")                                             # Ignatius Donnelly :contentReference[oaicite:20]{index=20}\n",
    "]\n",
    "\n",
    "create_dataset_from_list(example_list, \"sci-fi\", 100_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
