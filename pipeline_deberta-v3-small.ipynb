{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0f_3zEEMr4"
      },
      "source": [
        "# pipeline_deberta-v3-small.ipynb\n",
        "This notebook contains some template code to help you with loading/preprocessing the data.\n",
        "\n",
        "We start with some imports and constants.\n",
        "The training data is found in the `data` subfolder.\n",
        "There is also a tokenizer I've trained for you which you can use for the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCX9Tayrtu2E"
      },
      "source": [
        "Can be executed once — does not depend on the execution environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P7QGcWQwsygr"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 1. Clone the Repository (if required)\n",
        "# ========================================\n",
        "TOKEN = \"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\"  # ← GitHub Personal Access Token (PAT)  ⚠️\n",
        "# XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "!git clone https://{TOKEN}@github.com/KatsuhitoArasaka/BabyLM-Tiny.git  # your repository link  ⚠️\n",
        "%cd BabyLM-Tiny\n",
        "\n",
        "# ===============\n",
        "# 2. Set Paths\n",
        "# ===============\n",
        "TRAIN_PATH = './data/train.txt'       # Path to your training data  ⚠️\n",
        "DEV_PATH = './data/dev.txt'           # Path to your validation data  ⚠️\n",
        "SPM_PATH = './data/tokenizer.model'   # Path to your tokenizer model  ⚠️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n2o7lgau14Y"
      },
      "source": [
        "Execute after restarting the environment -- when changing the device (CPU ↔ GPU) you need to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "23L24AQHId8R"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 3. Install Dependencies\n",
        "# ==========================\n",
        "%pip install transformers datasets wandb trl bitsandbytes huggingface_hub --quiet\n",
        "\n",
        "\n",
        "# ======================================\n",
        "# 4. Authentication with wandb and hf\n",
        "# ======================================\n",
        "import wandb\n",
        "wandb.login()  # Enter your API key when prompted ⚠️\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()  # Paste your HF token from https://huggingface.co/settings/tokens  ⚠️\n",
        "# Token: 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\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 5. Import Libraries and Set Device\n",
        "# =====================================\n",
        "import subprocess\n",
        "import json\n",
        "import torch\n",
        "import datasets\n",
        "from functools import partial\n",
        "from datasets import load_dataset\n",
        "from transformers import DataCollatorForLanguageModeling, set_seed\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Check for GPU availability\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "em3xW7Rjf_pT"
      },
      "outputs": [],
      "source": [
        "# ================================================\n",
        "# 6. Start a new wandb run to track this script\n",
        "# ================================================\n",
        "run = wandb.init(\n",
        "    # Set the wandb entity where your project will be logged (generally your team name).\n",
        "    entity=\"Low-Resource_Pretraining\",\n",
        "    # Set the wandb project where this run will be logged.\n",
        "    project=\"NLP_LRP_BabyLM\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "    config={\n",
        "        \"learning_rate\": 0.02,  # the main parameter for configuring the optimizer\n",
        "        \"architecture\": \"DeBERTa\",  # a description of the model architecture, to track which model was used in the project\n",
        "        \"epochs\": 10,  # the number of training epochs, an important parameter for understanding the duration of the experiment and its settings\n",
        "\n",
        "        # \"dataset\": \"CIFAR-100\",\n",
        "        # \"batch_size\": 8,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glUE2dtEEMr8"
      },
      "source": [
        "Here are we load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RkhiZZGfEMr9"
      },
      "outputs": [],
      "source": [
        "# ===================\n",
        "# 7. Load Datasets\n",
        "# ===================\n",
        "\n",
        "# loading datasets\n",
        "# with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
        "#     train_data = [{\"text\": line.strip()} for line in f if line.strip()]\n",
        "\n",
        "# with open(DEV_PATH, 'r', encoding='utf-8') as f:\n",
        "#     val_data = [{\"text\": line.strip()} for line in f if line.strip()]\n",
        "\n",
        "# For using the 500k datasets, we can't split by new line, so we split by sentences.\n",
        "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
        "  full_text = f.read().strip()\n",
        "  sentences = [s.strip() for s in full_text.split('.') if s.strip()]\n",
        "  train_data = [{\"text\": s + \".\"} for s in sentences]  # add back the period if desired\n",
        "\n",
        "with open(DEV_PATH, 'r', encoding='utf-8') as f:\n",
        "  full_text = f.read().strip()\n",
        "  sentences = [s.strip() for s in full_text.split('.') if s.strip()]\n",
        "  val_data = [{\"text\": s + \".\"} for s in sentences]  # add back the period if desired\n",
        "\n",
        "# create DatasetDict object\n",
        "dataset = datasets.DatasetDict({\n",
        "    \"train\": datasets.Dataset.from_list(train_data),\n",
        "    \"validation\": datasets.Dataset.from_list(val_data)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sU2WHPCixD5m"
      },
      "outputs": [],
      "source": [
        "# =====================================================================================\n",
        "# 9. Setup Model, Training, and Logging (This is the part that may change per model)\n",
        "# =====================================================================================\n",
        "\n",
        "# Specific Settings for the Model you choose            ⚠️\n",
        "# Replace this block with model-specific settings       ⚠️\n",
        "model_name = \"microsoft/deberta-v3-small\"  # Example: DeBERTa model for MLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_config(config)\n",
        "\n",
        "# Data collator to support masked language modeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 10. Setup Training Arguments\n",
        "# ===============================\n",
        "# train.py\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    return pred_ids\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.flatten()\n",
        "    labels = labels.flatten()\n",
        "    mask = labels != -100\n",
        "    labels = labels[mask]\n",
        "    predictions = predictions[mask]\n",
        "\n",
        "    correct = labels == predictions\n",
        "    accuracy = correct.sum() / float(len(correct))\n",
        "    return {\"acc\": accuracy}\n",
        "\n",
        "# ======================\n",
        "# 11. Train the Model\n",
        "# ======================\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    train_dataset = dataset['train'],\n",
        "    eval_dataset = dataset['validation'],\n",
        "    data_collator = data_collator,\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "    compute_metrics=compute_metrics,\n",
        "    args = SFTConfig(\n",
        "        remove_unused_columns = True,\n",
        "        label_names = [\"labels\"],\n",
        "        dataset_num_proc = 12,\n",
        "        packing = True,\n",
        "        eval_packing = True,\n",
        "        max_seq_length = 64,\n",
        "        dataset_text_field = \"text\",\n",
        "        eval_strategy = \"steps\",\n",
        "        per_device_train_batch_size = 64,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_ratio = 0.05,\n",
        "        num_train_epochs = 10,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = True,\n",
        "        bf16 = False,\n",
        "        logging_steps = 10,\n",
        "        eval_steps = 100,\n",
        "        save_steps = 100,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 0,\n",
        "        # output_dir = \"\",\n",
        "        report_to = \"none\",\n",
        "        eval_accumulation_steps=1,\n",
        "        include_for_metrics=[],\n",
        "        max_grad_norm=1,\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oCemWTzJBP1p"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 12. Save the trained model locally (for later evaluation)\n",
        "# ============================================================\n",
        "\n",
        "# This will save the model to a folder inside your Colab environment\n",
        "# The folder will be deleted when the session ends unless uploaded elsewhere (e.g., Hugging Face, Google Drive)\n",
        "\n",
        "# save_path = \"./trained_models/model_name\"  # ⚠️ Change `model_name` if training multiple models in one session\n",
        "# trainer.save_model(save_path)        # Save model weights, config, etc.\n",
        "# tokenizer.save_pretrained(save_path) # Save tokenizer (required for evaluation)\n",
        "\n",
        "# OR use HF repository\n",
        "save_path = \"n1k1t427/deberta-v3-small-babylm_childes-default-dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AyEq4ozh44X2"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 13. Evaluate the model using BabyLM evaluation pipeline 2025\n",
        "# ===============================================================\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "\n",
        "!cp -r BabyLM-Tiny/evaluation_data evaluation-pipeline-2025/\n",
        "\n",
        "# --- Configuration ---\n",
        "EVAL_PIPELINE_REPO = \"evaluation-pipeline-2025\"\n",
        "TARGET_REPO_DIR = \"./\"  # Path to the BabyLM-Tiny repo (assumes you're already inside it)\n",
        "\n",
        "ARCHITECTURE = \"mlm\"  # \"mlm\", \"causal\", or \"mntp\"  # ⚠️\n",
        "EVAL_MODE = \"zero-shot-fast\"  # \"zero-shot\", \"zero-shot-fast\", or \"finetune\"  # ⚠️\n",
        "EVAL_DATA_FOLDER = \"fast_eval\"  # \"full_eval\" or \"fast_eval\" for \"zero-shot-fast\" # ⚠️\n",
        "# EVAL_DATA_FOLDER is ignored when using EVAL_MODE = \"finetune\"\n",
        "# because finetuning tasks load datasets directly from HuggingFace (e.g., GLUE), not from evaluation_data/.\n",
        "\n",
        "DATASET_NAME = \"childes-default-dataset\"  # the name of the dataset your model was trained on  # ⚠️\n",
        "ENABLE_GIT_PUSH = True  # set to True to automatically commit and push the result  # ⚠️\n",
        "\n",
        "model_id_path = pathlib.Path(save_path).name.replace(\"/\", \"_\")  # for results directory structure\n",
        "\n",
        "# --- Step 1: Clone evaluation pipeline if not present ---\n",
        "if not os.path.exists(EVAL_PIPELINE_REPO):\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/babylm/evaluation-pipeline-2025.git\"], check=True)\n",
        "\n",
        "# --- Step 2: Run the appropriate evaluation script ---\n",
        "if EVAL_MODE == \"zero-shot\":\n",
        "    eval_script = \"eval_zero_shot.sh\"\n",
        "    eval_dir = os.path.join(\"evaluation_data\", EVAL_DATA_FOLDER)\n",
        "    cmd = [\"bash\", eval_script, save_path, ARCHITECTURE, eval_dir]\n",
        "    working_dir = EVAL_PIPELINE_REPO\n",
        "\n",
        "elif EVAL_MODE == \"zero-shot-fast\":\n",
        "    eval_script = \"eval_zero_shot_fast.sh\"\n",
        "    revision_name = \"default\"  # can be adjusted for checkpoints\n",
        "    eval_dir = os.path.join(\"evaluation_data\", EVAL_DATA_FOLDER)\n",
        "    cmd = [\"bash\", eval_script, save_path, revision_name, ARCHITECTURE, eval_dir]\n",
        "    working_dir = EVAL_PIPELINE_REPO\n",
        "\n",
        "elif EVAL_MODE == \"finetune\":\n",
        "    eval_script = \"eval_finetune.sh\"\n",
        "    cmd = [\"bash\", eval_script, save_path]\n",
        "    working_dir = EVAL_PIPELINE_REPO\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown EVAL_MODE: {EVAL_MODE}\")\n",
        "\n",
        "subprocess.run(cmd, cwd=working_dir, check=True)\n",
        "\n",
        "# --- Step 3: Locate the results.txt file ---\n",
        "if EVAL_MODE.startswith(\"zero-shot\"):\n",
        "    eval_subfolder = \"zero_shot\"\n",
        "elif EVAL_MODE == \"finetune\":\n",
        "    eval_subfolder = \"finetune\"\n",
        "else:\n",
        "    raise RuntimeError(\"Unexpected EVAL_MODE value\")\n",
        "\n",
        "results_root = pathlib.Path(EVAL_PIPELINE_REPO) / \"results\" / model_id_path / \"main\" / eval_subfolder\n",
        "\n",
        "# Search recursively for results.txt\n",
        "candidate = None\n",
        "for root, _, files in os.walk(results_root):\n",
        "    for f in files:\n",
        "        if f == \"results.txt\":\n",
        "            candidate = os.path.join(root, f)\n",
        "            break\n",
        "if not candidate:\n",
        "    raise FileNotFoundError(\"Evaluation completed, but results.txt was not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- if wandb run hasn't been launched yet ---\n",
        "run = wandb.init(\n",
        "    name=f\"{model_id_path}_{DATASET_NAME}_{EVAL_MODE}\",\n",
        "    entity=\"Low-Resource_Pretraining\",\n",
        "    project=\"NLP_LRP_BabyLM\",\n",
        "    config={\n",
        "        \"learning_rate\": 0.02,\n",
        "        \"architecture\": \"DeBERTa\",\n",
        "        \"epochs\": 10,\n",
        "    },\n",
        ")\n"
      ],
      "metadata": {
        "id": "JjJJ3jAL_6qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Log evaluation results to wandb ---\n",
        "with open(candidate, \"r\") as f:\n",
        "    lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Construct a clear log name: <model_id>_<dataset>_<eval_mode>\n",
        "log_name = f\"{model_id_path}_{DATASET_NAME}_{EVAL_MODE}\"\n",
        "wandb.log({log_name: wandb.Html(\"<br>\".join(lines))})\n",
        "\n",
        "# --- Step 5: Save results using <dataset>_<eval_mode>.txt format ---\n",
        "output_dir = (\n",
        "    pathlib.Path(TARGET_REPO_DIR)\n",
        "    / \"models_evaluation_results\"\n",
        "    / \"deberta-v3-small_results\"  # ⚠️\n",
        ")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "result_file_path = output_dir / f\"{DATASET_NAME}_{EVAL_MODE}.txt\"\n",
        "shutil.copy(candidate, result_file_path)\n",
        "\n",
        "print(f\"✅ Evaluation complete. Results saved to {result_file_path}\")\n",
        "\n",
        "# --- Step 6: Optionally commit and push the result to GitHub ---\n",
        "if ENABLE_GIT_PUSH:\n",
        "    subprocess.run([\"git\", \"add\", str(result_file_path)], cwd=TARGET_REPO_DIR)\n",
        "    subprocess.run([\"git\", \"commit\", \"-m\", f\"Add {EVAL_MODE} evaluation results for {DATASET_NAME}\"], cwd=TARGET_REPO_DIR)\n",
        "    subprocess.run([\"git\", \"push\"], cwd=TARGET_REPO_DIR)"
      ],
      "metadata": {
        "id": "2ZDh_r7_D-S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVSCNJXvSV3i"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# 14. Upload Trained Model to Hugging Face Hub\n",
        "# ===============================================\n",
        "\n",
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "hf_repo_name = \"deberta-v3-small-babylm\"  # Change this to a unique name for your model       ⚠️\n",
        "hf_username = \"n1k1t427\"          # Replace with your actual Hugging Face username    ⚠️\n",
        "repo_id = f\"{hf_username}/{hf_repo_name}\"\n",
        "\n",
        "# Create a public repo (use private=True if needed)\n",
        "create_repo(repo_id, exist_ok=True, private=True)\n",
        "\n",
        "# Upload entire trained model folder\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=save_path,\n",
        "    path_in_repo=\".\",  # Upload everything from the folder\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded to: https://huggingface.co/{repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qstKQ5d6S692"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 15. Finish wandb Logging\n",
        "# ===========================\n",
        "wandb.finish()  # log the final metrics and mark the run as complete"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}