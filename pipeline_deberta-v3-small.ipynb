{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0f_3zEEMr4"
      },
      "source": [
        "# pipeline_deberta-v3-small.ipynb\n",
        "This notebook contains some template code to help you with loading/preprocessing the data.\n",
        "\n",
        "We start with some imports and constants.\n",
        "The training data is found in the `data` subfolder.\n",
        "There is also a tokenizer I've trained for you which you can use for the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCX9Tayrtu2E"
      },
      "source": [
        "Can be executed once — does not depend on the execution environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P7QGcWQwsygr"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 1. Clone the Repository (if required)\n",
        "# ========================================\n",
        "TOKEN = \"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\"  # ← GitHub Personal Access Token (PAT)  ⚠️\n",
        "# XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "!git clone https://{TOKEN}@github.com/KatsuhitoArasaka/BabyLM-Tiny.git  # your repository link  ⚠️\n",
        "%cd BabyLM-Tiny\n",
        "\n",
        "# ===============\n",
        "# 2. Set Paths\n",
        "# ===============\n",
        "TRAIN_PATH = './data/train.txt'       # Path to your training data  ⚠️\n",
        "DEV_PATH = './data/dev.txt'           # Path to your validation data  ⚠️\n",
        "SPM_PATH = './data/tokenizer.model'   # Path to your tokenizer model  ⚠️\n",
        "# Path to evaluating scripts\n",
        "BLIMP_SCRIPT = \"./evaluate_blimp.py\"\n",
        "GLUE_SCRIPT = \"./evaluate_glue.py\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n2o7lgau14Y"
      },
      "source": [
        "Execute after restarting the environment -- when changing the device (CPU ↔ GPU) you need to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "23L24AQHId8R"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 3. Install Dependencies\n",
        "# ==========================\n",
        "%pip install transformers datasets wandb trl bitsandbytes huggingface_hub --quiet\n",
        "\n",
        "# ======================================\n",
        "# 4. Authentication with wandb and hf\n",
        "# ======================================\n",
        "import wandb\n",
        "wandb.login()  # Enter your API key when prompted ⚠️\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()  # Paste your HF token from https://huggingface.co/settings/tokens  ⚠️\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 5. Import Libraries and Set Device\n",
        "# =====================================\n",
        "import subprocess\n",
        "import json\n",
        "import torch\n",
        "import datasets\n",
        "from functools import partial\n",
        "from datasets import load_dataset\n",
        "from transformers import DataCollatorForLanguageModeling, set_seed\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Check for GPU availability\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "em3xW7Rjf_pT"
      },
      "outputs": [],
      "source": [
        "# ================================================\n",
        "# 6. Start a new wandb run to track this script\n",
        "# ================================================\n",
        "run = wandb.init(\n",
        "    # Set the wandb entity where your project will be logged (generally your team name).\n",
        "    entity=\"Low-Resource_Pretraining\",\n",
        "    # Set the wandb project where this run will be logged.\n",
        "    project=\"NLP_LRP_BabyLM\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "    config={\n",
        "        \"learning_rate\": 0.02,  # the main parameter for configuring the optimizer\n",
        "        \"architecture\": \"DeBERTa\",  # a description of the model architecture, to track which model was used in the project\n",
        "        \"epochs\": 10,  # the number of training epochs, an important parameter for understanding the duration of the experiment and its settings\n",
        "\n",
        "        # \"dataset\": \"CIFAR-100\",\n",
        "        # \"batch_size\": 8,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glUE2dtEEMr8"
      },
      "source": [
        "Here are we load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RkhiZZGfEMr9"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 7. Load Datasets\n",
        "# ================================\n",
        "\n",
        "# loading datasets\n",
        "# with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
        "#     train_data = [{\"text\": line.strip()} for line in f if line.strip()]\n",
        "\n",
        "# with open(DEV_PATH, 'r', encoding='utf-8') as f:\n",
        "#     val_data = [{\"text\": line.strip()} for line in f if line.strip()]\n",
        "\n",
        "# For using the 500k datasets, we can't split by new line, so we split by sentences.\n",
        "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
        "  full_text = f.read().strip()\n",
        "  sentences = [s.strip() for s in full_text.split('.') if s.strip()]\n",
        "  train_data = [{\"text\": s + \".\"} for s in sentences]  # add back the period if desired\n",
        "\n",
        "with open(DEV_PATH, 'r', encoding='utf-8') as f:\n",
        "  full_text = f.read().strip()\n",
        "  sentences = [s.strip() for s in full_text.split('.') if s.strip()]\n",
        "  val_data = [{\"text\": s + \".\"} for s in sentences]  # add back the period if desired\n",
        "\n",
        "# create DatasetDict object\n",
        "dataset = datasets.DatasetDict({\n",
        "    \"train\": datasets.Dataset.from_list(train_data),\n",
        "    \"validation\": datasets.Dataset.from_list(val_data)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sU2WHPCixD5m"
      },
      "outputs": [],
      "source": [
        "# =====================================================================================\n",
        "# 9. Setup Model, Training, and Logging (This is the part that may change per model)\n",
        "# =====================================================================================\n",
        "\n",
        "# Specific Settings for the Model you choose            ⚠️\n",
        "# Replace this block with model-specific settings       ⚠️\n",
        "model_name = \"microsoft/deberta-v3-small\"  # Example: DeBERTa model for MLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_config(config)\n",
        "\n",
        "# Data collator to support masked language modeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 10. Setup Training Arguments\n",
        "# ===============================\n",
        "# train.py\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    return pred_ids\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.flatten()\n",
        "    labels = labels.flatten()\n",
        "    mask = labels != -100\n",
        "    labels = labels[mask]\n",
        "    predictions = predictions[mask]\n",
        "\n",
        "    correct = labels == predictions\n",
        "    accuracy = correct.sum() / float(len(correct))\n",
        "    return {\"acc\": accuracy}\n",
        "\n",
        "# ======================\n",
        "# 11. Train the Model\n",
        "# ======================\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    train_dataset = dataset['train'],\n",
        "    eval_dataset = dataset['validation'],\n",
        "    data_collator = data_collator,\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "    compute_metrics=compute_metrics,\n",
        "    args = SFTConfig(\n",
        "        remove_unused_columns = True,\n",
        "        label_names = [\"labels\"],\n",
        "        dataset_num_proc = 12,\n",
        "        packing = True,\n",
        "        eval_packing = True,\n",
        "        max_seq_length = 64,\n",
        "        dataset_text_field = \"text\",\n",
        "        eval_strategy = \"steps\",\n",
        "        per_device_train_batch_size = 64,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_ratio = 0.05,\n",
        "        num_train_epochs = 10,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = True,\n",
        "        bf16 = False,\n",
        "        logging_steps = 10,\n",
        "        eval_steps = 100,\n",
        "        save_steps = 100,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 0,\n",
        "        # output_dir = \"\",\n",
        "        report_to = \"none\",\n",
        "        eval_accumulation_steps=1,\n",
        "        include_for_metrics=[],\n",
        "        max_grad_norm=1,\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oCemWTzJBP1p"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 12. Save the trained model locally (for later evaluation)\n",
        "# ============================================================\n",
        "\n",
        "# This will save the model to a folder inside your Colab environment\n",
        "# The folder will be deleted when the session ends unless uploaded elsewhere (e.g., Hugging Face, Google Drive)\n",
        "save_path = \"./trained_models/model_name\"  # ⚠️ Change `model_name` if training multiple models in one session\n",
        "trainer.save_model(save_path)        # Save model weights, config, etc.\n",
        "tokenizer.save_pretrained(save_path) # Save tokenizer (required for evaluation)\n",
        "\n",
        "# OR use HF repository\n",
        "save_path = \"n1k1t427/deberta-v3-small-babylm_childes-default-dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AyEq4ozh44X2"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 13. Evaluate the model: BLiMP\n",
        "# ========================================\n",
        "\n",
        "MODEL_TYPE = \"encoder\"  # or \"decoder\", if you train autoregressive model       ⚠️\n",
        "\n",
        "# ---- Run BLiMP Evaluation ----\n",
        "# add \"--local_only\" argument if using Colab folder.  ⚠️\n",
        "blimp_output = subprocess.run(\n",
        "    [\"python\", BLIMP_SCRIPT,\n",
        "     \"--model_type\", MODEL_TYPE,\n",
        "     \"--model_path\", save_path,\n",
        "     \"--batch_size\", \"16\"],  # For Colab (T4/P100): usually 16 or 32.\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "# Parse BLiMP results from stdout\n",
        "blimp_lines = blimp_output.stdout.strip().split(\"\\n\")\n",
        "blimp_results = {line.split(\":\")[0].strip(\" -\"): float(line.split(\":\")[1]) for line in blimp_lines if \":\" in line}\n",
        "blimp_avg = blimp_results.get(\"Average\", sum(blimp_results.values()) / len(blimp_results))\n",
        "\n",
        "# ---- Log BLiMP to wandb ----\n",
        "wandb.log({\"blimp_avg\": blimp_avg, \"blimp_details\": wandb.Html(blimp_output.stdout.replace('\\n', '<br>'))})\n",
        "\n",
        "# ---- Save BLiMP results ----\n",
        "# Don't forget to create folder for a corresponding model, if not created                       ⚠️\n",
        "# and replace `modelname` in 'modelname_results' and `dataset_date` in 'blimp_dataset_date',\n",
        "# otherwise you may accidentally overwrite the results from another model or won't be able to save them.\n",
        "with open(\"models_evaluation_results/modelname_results/blimp_dataset_date.json\", \"w\") as f:\n",
        "    json.dump(blimp_results, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvLfGh_nBd9u"
      },
      "outputs": [],
      "source": [
        "# ==========\n",
        "#    GLUE\n",
        "# ==========\n",
        "# ---- Run GLUE Evaluation ----\n",
        "glue_subsets = ['cola','sst2', 'mrpc', 'qnli', 'rte', 'boolq', 'multirc']\n",
        "glue_scores = {}\n",
        "\n",
        "# add \"--local_only\" argument if using Colab folder.  ⚠️\n",
        "for subset in glue_subsets:\n",
        "    glue_eval = subprocess.run(\n",
        "        [\"python\", GLUE_SCRIPT,\n",
        "         \"--subset\", subset,\n",
        "         \"--model_type\", MODEL_TYPE,\n",
        "         \"--model_path\", save_path],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "\n",
        "# Parse from print(\"Epoch: x, Result: ...\") → take the last line\n",
        "for line in glue_eval.stdout.strip().split(\"\\n\")[::-1]:\n",
        "    if \"Best result:\" in line:\n",
        "        glue_scores[subset] = float(line.split(\":\")[1])\n",
        "        break\n",
        "\n",
        "# ---- Log GLUE to wandb ----\n",
        "wandb.log({\"glue_avg\": sum(glue_scores.values()) / len(glue_scores), **{f\"glue_{k}\": v for k, v in glue_scores.items()}})\n",
        "\n",
        "# ---- Save GLUE results ----\n",
        "# Don't forget to replace `results_glue_modelname` in the file name with the model name or date,        ⚠️\n",
        "# otherwise you may accidentally overwrite the results from another model.\n",
        "with open(\"models_evaluation_results/modelname_results/glue_dataset_date.json\", \"w\") as f:\n",
        "    json.dump(glue_scores, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVSCNJXvSV3i"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# 14. Upload Trained Model to Hugging Face Hub\n",
        "# ===============================================\n",
        "\n",
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "hf_repo_name = \"deberta-v3-small-babylm\"  # Change this to a unique name for your model       ⚠️\n",
        "hf_username = \"n1k1t427\"          # Replace with your actual Hugging Face username    ⚠️\n",
        "repo_id = f\"{hf_username}/{hf_repo_name}\"\n",
        "\n",
        "# Create a public repo (use private=True if needed)\n",
        "create_repo(repo_id, exist_ok=True, private=True)\n",
        "\n",
        "# Upload entire trained model folder\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=save_path,\n",
        "    path_in_repo=\".\",  # Upload everything from the folder\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded to: https://huggingface.co/{repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qstKQ5d6S692"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 15. Finish wandb Logging\n",
        "# ===========================\n",
        "wandb.finish()  # log the final metrics and mark the run as complete"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
