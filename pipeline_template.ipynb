{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0f_3zEEMr4"
      },
      "source": [
        "# template.ipynb\n",
        "This notebook contains some template code to help you with loading/preprocessing the data.\n",
        "\n",
        "We start with some imports and constants.\n",
        "The training data is found in the `data` subfolder.\n",
        "There is also a tokenizer I've trained for you which you can use for the project."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can be executed once — does not depend on the execution environment."
      ],
      "metadata": {
        "id": "WCX9Tayrtu2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 1. Clone the Repository (if required)\n",
        "# ========================================\n",
        "TOKEN = \"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\"  # ← GitHub Personal Access Token (PAT)  ⚠️\n",
        "# XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "!git clone https://{TOKEN}@github.com/KatsuhitoArasaka/BabyLM-Tiny.git  # your repository link  ⚠️\n",
        "%cd BabyLM-Tiny\n",
        "\n",
        "# ===============\n",
        "# 2. Set Paths\n",
        "# ===============\n",
        "TRAIN_PATH = './data/train.txt'       # Path to your training data  ⚠️\n",
        "DEV_PATH = './data/dev.txt'           # Path to your validation data  ⚠️\n",
        "SPM_PATH = './data/tokenizer.model'   # Path to your tokenizer model  ⚠️\n",
        "# Path to evaluating scripts\n",
        "BLIMP_SCRIPT = \"./evaluate_blimp.py\"\n",
        "GLUE_SCRIPT = \"./evaluate_glue.py\""
      ],
      "metadata": {
        "id": "P7QGcWQwsygr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute after restarting the environment -- when changing the device (CPU ↔ GPU) you need to restart the kernel."
      ],
      "metadata": {
        "id": "4n2o7lgau14Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 3. Install Dependencies\n",
        "# ==========================\n",
        "!pip install transformers datasets wandb --quiet\n",
        "\n",
        "# ===============================\n",
        "# 4. Authentication with wandb\n",
        "# ===============================\n",
        "import wandb\n",
        "wandb.login()  # Enter your API key when prompted ⚠️\n",
        "\n",
        "# =====================================\n",
        "# 5. Import Libraries and Set Device\n",
        "# =====================================\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import datasets\n",
        "from functools import partial\n",
        "from datasets import load_dataset\n",
        "# from transformers import DebertaV2Tokenizer as Tokenizer\n",
        "# from transformers.models.deberta_v2.tokenization_deberta_v2 import DebertaV2Tokenizer as Tokenizer\n",
        "from transformers import AutoTokenizer # for custom tokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Check for GPU availability\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "23L24AQHId8R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 6. Start a new wandb run to track this script\n",
        "# ================================================\n",
        "run = wandb.init(\n",
        "    # Set the wandb entity where your project will be logged (generally your team name).\n",
        "    entity=\"Low-Resource_Pretraining\",\n",
        "    # Set the wandb project where this run will be logged.\n",
        "    project=\"NLP_LRP_BabyLM\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "    config={\n",
        "        \"learning_rate\": 0.02,  # the main parameter for configuring the optimizer\n",
        "        \"architecture\": \"DeBERTa\",  # a description of the model architecture, to track which model was used in the project\n",
        "        \"epochs\": 10,  # the number of training epochs, an important parameter for understanding the duration of the experiment and its settings\n",
        "\n",
        "        # \"dataset\": \"CIFAR-100\",\n",
        "        # \"batch_size\": 8,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "em3xW7Rjf_pT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glUE2dtEEMr8"
      },
      "source": [
        "Here are we load the dataset and tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkhiZZGfEMr9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 7. Load Dataset and Tokenizer\n",
        "# ================================\n",
        "\n",
        "#dataset = datasets.load_dataset('text', data_files={'train': TRAIN_PATH, 'validation': DEV_PATH}) # loads the dataset\n",
        "\n",
        "# loading datasets\n",
        "with open(TRAIN_PATH, 'r', encoding='utf-8') as f:\n",
        "    train_data = [{\"text\": line.strip()} for line in f if line.strip()]\n",
        "\n",
        "with open(DEV_PATH, 'r', encoding='utf-8') as f:\n",
        "    val_data = [{\"text\": line.strip()} for line in f if line.strip()]\n",
        "\n",
        "# create DatasetDict object\n",
        "dataset = datasets.DatasetDict({\n",
        "    \"train\": datasets.Dataset.from_list(train_data),\n",
        "    \"validation\": datasets.Dataset.from_list(val_data)\n",
        "})\n",
        "\n",
        "\n",
        "# tokenizer = Tokenizer.from_pretrained(SPM_PATH) # loads the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(SPM_PATH) # loads custom tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H36tyN0PEMr9"
      },
      "source": [
        "Next we need to tokenize the data. I've split this into 2 functions, 1 for an encoder model, 1 for a decoder model.\n",
        "These functions are written to take a dictionary of lists, and return the same. This is specifically to allow us to use the `map()` function in the `datasets` library, which we show below later.\n",
        "\n",
        "### Tokenization (Encoder)\n",
        "For encoder tokenization, we will be doing MLM, and masking is applied randomly and dynamically during training. Right now, the function below is a preprocessing step, and thus only applied once. So we skip masking for now, and just say the input and labels are the same.\n",
        "\n",
        "The argument `add_special_tokens` is set to `True` (it is also true by default, but I put it here for clarity), which means that special tokens that go at the start and end of the text will be added. [CLS] and [SEP] are special tokens marking the beginning and end of the text. So, e.g. \"The sky is blue.\" will become \"[CLS] The sky is blue. [SEP]\"\n",
        "\n",
        "### Tokenization (Decoder)\n",
        "For decoder tokenization, we will be doing CLM, so predicting the next token. Therefore we need to offset the labels from the inputs so that the 1st token is used to predict the 2nd, etc. See the paragraph above for an explanation of [CLS] and [SEP]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pw-0QO72EMr-"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# 8. Tokenization Function (Encoder/Decoder)\n",
        "# =============================================\n",
        "def tokenize_encoder(examples, tokenizer, max_length):\n",
        "    batch = {\n",
        "        \"input_ids\": [],\n",
        "        \"labels\": [],\n",
        "    }\n",
        "\n",
        "    for example in examples[\"text\"]:\n",
        "        tokens = tokenizer.encode(example,\n",
        "                                  add_special_tokens=True,\n",
        "                                  padding=\"max_length\",\n",
        "                                  truncation=True,\n",
        "                                  max_length=max_length) # will add [CLS] and [SEP]\n",
        "        batch[\"input_ids\"].append(tokens)\n",
        "        batch[\"labels\"].append(tokens)\n",
        "\n",
        "    return batch\n",
        "\n",
        "def tokenize_decoder(examples, tokenizer, max_length):\n",
        "    batch = {\n",
        "        \"input_ids\": [],\n",
        "        \"labels\": [],\n",
        "    }\n",
        "\n",
        "    for example in examples[\"text\"]:\n",
        "        tokens = tokenizer.encode(example,\n",
        "                                  add_special_tokens=True,\n",
        "                                  padding=\"max_length\",\n",
        "                                  truncation=True,\n",
        "                                  max_length=max_length) # will add [CLS] and [SEP]\n",
        "        batch[\"input_ids\"].append(tokens[:-1])\n",
        "        batch[\"labels\"].append(tokens[1:])\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F94j3RR6EMr-"
      },
      "source": [
        "Now we can apply the tokenization. I show it for `tokenize_encoder` but it is the same for `tokenize_decoder`. First the function needs to only take 1 parameter, which is fine because our tokenizer is constant, so we can just apply `functools.partial`. Next, we apply `map()`, which allows fast parallel preprocessing of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2mZPNAjEMr_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Apply the tokenization to the dataset\n",
        "tokenize_fn = partial(tokenize_encoder, tokenizer = tokenizer, max_length = 128) # need to make the function unary for map()\n",
        "tokenized_dataset = dataset.map(tokenize_fn, batched = True, num_proc = 4, remove_columns = ['text']) # map works with functions that return a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# 9. Setup Model, Training, and Logging (This is the part that may change per model)\n",
        "# =====================================================================================\n",
        "\n",
        "# Specific Settings for the Model you choose            ⚠️\n",
        "# Replace this block with model-specific settings       ⚠️\n",
        "model_name = \"microsoft/deberta-v3-small\"  # Example: DeBERTa model for MLM\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)  # Load a Masked Language Model (MLM)\n",
        "\n",
        "# Data collator to support masked language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "# ===============================\n",
        "# 10. Setup Training Arguments\n",
        "# ===============================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # Directory to save the model\n",
        "    eval_strategy=\"steps\",         # Evaluation strategy (can be \"steps\" or \"epoch\")\n",
        "    eval_steps=2000,                      # Steps between evaluations\n",
        "    logging_dir=\"./logs\",                # Directory to save logs\n",
        "    logging_steps=1000,                   # Steps between logging\n",
        "    per_device_train_batch_size=8,       # Batch size for training\n",
        "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
        "    num_train_epochs=3,                  # Number of training epochs\n",
        "    save_steps=2000,                     # Save model after every n steps\n",
        "    save_total_limit=3,                  # Limit the number of saved models\n",
        "    load_best_model_at_end=True,         # Load the best model at the end of training\n",
        "    fp16=True,\n",
        "    learning_rate=3e-5, # you can experiment with 3e-5 to 1e-4\n",
        "    weight_decay=0.01, # regularization, helps to avoid overfitting\n",
        "    warmup_steps=500, # warm-up for a smooth start\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 11. Train the Model\n",
        "# ======================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sU2WHPCixD5m",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 12. Save the trained model locally (for later evaluation)\n",
        "# ============================================================\n",
        "\n",
        "# This will save the model to a folder inside your Colab environment\n",
        "# The folder will be deleted when the session ends unless uploaded elsewhere (e.g., Hugging Face, Google Drive)\n",
        "save_path = \"./trained_models/model_name\"  # ⚠️ Change `model_name` if training multiple models in one session\n",
        "\n",
        "trainer.save_model(save_path)        # Save model weights, config, etc.\n",
        "tokenizer.save_pretrained(save_path) # Save tokenizer (required for evaluation)"
      ],
      "metadata": {
        "id": "oCemWTzJBP1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 13. Evaluate the model (BLiMP + GLUE)\n",
        "# ========================================\n",
        "\n",
        "MODEL_TYPE = \"encoder\"  # or \"decoder\", if you train autoregressive model       ⚠️\n",
        "\n",
        "# ---- Run BLiMP Evaluation ----\n",
        "blimp_output = subprocess.run(\n",
        "    [\"python\", BLIMP_SCRIPT,\n",
        "     \"--model_type\", MODEL_TYPE,\n",
        "     \"--model_path\", save_path,\n",
        "     \"--batch_size\", \"16\"],  # For Colab (T4/P100): usually 16 or 32.\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "# Parse BLiMP results from stdout\n",
        "blimp_lines = blimp_output.stdout.strip().split(\"\\n\")\n",
        "blimp_results = {line.split(\":\")[0].strip(\" -\"): float(line.split(\":\")[1]) for line in blimp_lines if \":\" in line}\n",
        "blimp_avg = blimp_results.get(\"Average\", sum(blimp_results.values()) / len(blimp_results))\n",
        "\n",
        "# ---- Log BLiMP to wandb ----\n",
        "wandb.log({\"blimp_avg\": blimp_avg, \"blimp_details\": wandb.Html(blimp_output.stdout.replace('\\n', '<br>'))})\n",
        "\n",
        "# ---- Save BLiMP results ----\n",
        "# Don't forget to create folder for a corresponding model, if not created                       ⚠️\n",
        "# and replace `modelname` in 'modelname_results' and `dataset_date` in 'blimp_dataset_date',\n",
        "# otherwise you may accidentally overwrite the results from another model or won't be able to save them.\n",
        "with open(\"models_evaluation_results/modelname_results/blimp_dataset_date.json\", \"w\") as f:\n",
        "    json.dump(blimp_results, f, indent=2)\n",
        "\n",
        "\n",
        "# ---- Run GLUE Evaluation (по одному сабсету) ----\n",
        "glue_subsets = ['cola','sst2', 'mrpc', 'qnli', 'rte', 'boolq', 'multirc']\n",
        "glue_scores = {}\n",
        "\n",
        "for subset in glue_subsets:\n",
        "    glue_eval = subprocess.run(\n",
        "        [\"python\", GLUE_SCRIPT,\n",
        "         \"--subset\", subset,\n",
        "         \"--model_type\", MODEL_TYPE,\n",
        "         \"--model_path\", save_path],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "\n",
        "# Parse from print(\"Epoch: x, Result: ...\") → take the last line\n",
        "for line in glue_eval.stdout.strip().split(\"\\n\")[::-1]:\n",
        "    if \"Best result:\" in line:\n",
        "        glue_scores[subset] = float(line.split(\":\")[1])\n",
        "        break\n",
        "\n",
        "# ---- Log GLUE to wandb ----\n",
        "wandb.log({\"glue_avg\": sum(glue_scores.values()) / len(glue_scores), **{f\"glue_{k}\": v for k, v in glue_scores.items()}})\n",
        "\n",
        "# ---- Save GLUE results ----\n",
        "# Don't forget to replace `results_glue_modelname` in the file name with the model name or date,        ⚠️\n",
        "# otherwise you may accidentally overwrite the results from another model.\n",
        "with open(\"models_evaluation_results/modelname_results/glue_dataset_date.json\", \"w\") as f:\n",
        "    json.dump(glue_scores, f, indent=2)\n"
      ],
      "metadata": {
        "id": "AyEq4ozh44X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# 14. Upload Trained Model to Hugging Face Hub\n",
        "# ===============================================\n",
        "\n",
        "# Install and login (only needs to be done once)\n",
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login()  # Paste your HF token from https://huggingface.co/settings/tokens\n",
        "\n",
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "hf_repo_name = \"deberta-small-babylm\"  # Change this to a unique name for your model       ⚠️\n",
        "hf_username = \"your_username\"          # Replace with your actual Hugging Face username    ⚠️\n",
        "repo_id = f\"{hf_username}/{hf_repo_name}\"\n",
        "\n",
        "# Create a public repo (use private=True if needed)\n",
        "create_repo(repo_id, exist_ok=True, private=True)\n",
        "\n",
        "# Upload entire trained model folder\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=save_path,\n",
        "    path_in_repo=\".\",  # Upload everything from the folder\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded to: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "rVSCNJXvSV3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 15. Finish wandb Logging\n",
        "# ===========================\n",
        "wandb.finish()  # log the final metrics and mark the run as complete"
      ],
      "metadata": {
        "id": "qstKQ5d6S692"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
